# DeepX [![Build Status](https://travis-ci.org/sharadmv/deepx.svg)](https://travis-ci.org/sharadmv/deepx) [![Coverage Status](https://coveralls.io/repos/sharadmv/deepx/badge.svg?branch=master&service=github)](https://coveralls.io/github/sharadmv/deepx?branch=master)
DeepX is a deep learning library designed with flexibility and succinctness in mind.
The key aspect is an expressive shorthand to describe your neural network architecture.

Installation
====================================

```
$ pip install deepx
```

Quickstart
=================================

The first step in building your first network is to define your *architecture*.
The architecture is the input-output structure of your network.
Let's consider the task of classifying MNIST with a multilayer perceptron (MLP).

```python
$ from deepx.nn import *
$ from deepx.optimize import *
$ mlp = Vector(784) >> Tanh(200) >> Tanh(200) >> Softmax(10)
```

Sweet! We now have an architecture that can classify MNIST! The next step is defining
how we use the architecture. Simply put, an architecture plus a set of methods is a *model*.

```python
$ model = mlp | (predict, cross_entropy, rmsprop)
```

We now have a `model` object which contains three methods: `predict`, `loss`, and `train`, each of which
is generated by the three elements of the tuple. `predict` takes a batch of vectors and
passes it through the network, returning the softmax probabilities of each class. `loss` takes in a
batch of vectors and a batch of one-hot encoded labels and computes the cross-entropy loss.
`train` takes in the same parameters as `loss` and performs an `rmsprop` gradient update.

That's it, we're done!
